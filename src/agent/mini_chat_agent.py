"""
Mini Chat Agent for video content retrieval and Q&A
"""

import logging
import random
from typing import List, Dict, Any, Optional
from pathlib import Path
import json
import re

from openai import OpenAI
from config.settings import settings
from src.embedding.embedding_generator import VideoProcessor


logger = logging.getLogger(__name__)


class ClarificationMode:
    """Nachfrage-Modus fÃ¼r unspezifische Fragen"""
    
    def __init__(self, openai_client, video_processor):
        self.openai_client = openai_client
        self.video_processor = video_processor
        self.clarification_history = []
        self.iterative_mode = False  # One-question-at-a-time mode
        
    def is_question_too_vague(self, question: str) -> bool:
        """Erkennt, ob eine Frage zu unspezifisch ist"""
        
        # Typische unspezifische Muster
        vague_patterns = [
            r"ich mÃ¶chte (.*)",  # "ich mÃ¶chte abnehmen"
            r"ich will (.*)",     # "ich will mehr Leads"
            r"ich brauche (.*)",  # "ich brauche Hilfe"
            r"wie kann ich (.*)", # "wie kann ich erfolgreich sein"
            r"was soll ich (.*)", # "was soll ich tun"
            r"hilf mir (.*)",     # "hilf mir"
            r"ich habe ein problem", # "ich habe ein Problem"
            r"ich weiÃŸ nicht (.*)", # "ich weiÃŸ nicht was"
        ]
        
        question_lower = question.lower().strip()
        
        # PrÃ¼fe auf unspezifische Muster
        for pattern in vague_patterns:
            if re.search(pattern, question_lower):
                return True
        
        # PrÃ¼fe auf sehr kurze oder generische Fragen
        if len(question.split()) <= 3:
            return True
            
        # PrÃ¼fe auf generische WÃ¶rter ohne Kontext
        generic_words = ["hilfe", "problem", "tun", "machen", "erfolg", "besser", "mehr", "weniger"]
        if any(word in question_lower for word in generic_words) and len(question.split()) <= 5:
            return True
            
        return False
    
    def is_question_specific_enough(self, question: str) -> bool:
        """Erkennt, ob eine Frage spezifisch genug ist fÃ¼r eine Antwort"""
        
        question_lower = question.lower().strip()
        
        # Spezifische Indikatoren
        specific_indicators = [
            "kg", "kilo", "woche", "tag", "stunde", "minute",  # Zeit/Gewicht
            "sport", "training", "laufen", "fitness", "gym",   # Sport
            "fleisch", "gemÃ¼se", "obst", "wasser", "kalorien", # ErnÃ¤hrung
            "leads", "kunden", "verkauf", "marketing", "social media", # Business
            "euro", "dollar", "budget", "kosten", "preis",     # Finanzen
            "team", "mitarbeiter", "angestellte", "freelancer", # Personal
            "website", "online", "shop", "app", "software"     # Technologie
        ]
        
        # ZÃ¤hle spezifische Indikatoren
        specific_count = sum(1 for indicator in specific_indicators if indicator in question_lower)
        
        # Frage ist spezifisch genug wenn:
        # 1. Mindestens 2 spezifische Indikatoren vorhanden sind
        # 2. Oder die Frage lÃ¤nger als 10 WÃ¶rter ist
        # 3. Oder konkrete Zahlen enthalten sind
        has_numbers = bool(re.search(r'\d+', question))
        
        return (specific_count >= 2 or 
                len(question.split()) > 10 or 
                has_numbers)
    
    def generate_clarification_questions(self, question: str, context_chunks: List[Dict[str, Any]]) -> str:
        """Generiert spezifische Nachfragen basierend auf der ursprÃ¼nglichen Frage und dem Kontext"""
        
        # Baue Kontext fÃ¼r die Nachfrage-Generierung
        context_text = self._build_context_for_clarification(context_chunks)
        
        clarification_prompt = f"""Du bist ein erfahrener Coach, der gezielt nachfragt, um spezifische Antworten zu geben.

UrsprÃ¼ngliche Frage: "{question}"

VerfÃ¼gbarer Kontext aus den Videos:
{context_text}

Deine Aufgabe: Erkenne, was in der ursprÃ¼nglichen Frage zu unspezifisch ist und stelle 3-5 gezielte Nachfragen, die dem Fragesteller helfen, seine Frage zu prÃ¤zisieren. Nutze dabei den verfÃ¼gbaren Kontext, um relevante Nachfragen zu stellen.

Beispiele fÃ¼r gute Nachfragen:
- Bei "ich mÃ¶chte abnehmen": "Wie viel mÃ¶chtest du abnehmen? Wie viel Sport machst du aktuell? Wie ernÃ¤hrst du dich gerade? Welche DiÃ¤ten hast du schon probiert?"
- Bei "ich mÃ¶chte mehr Leads": "FÃ¼r welches Produkt/Service mÃ¶chtest du mehr Leads? In welchem Bereich arbeitest du? Was machst du bereits fÃ¼r Lead-Generierung?"

Stelle die Nachfragen in einem freundlichen, aber direkten Ton. Verwende "du" und sei konkret.
Antworte NUR mit den Nachfragen, keine zusÃ¤tzlichen ErklÃ¤rungen."""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",  # Verwende GPT-4o fÃ¼r bessere Nachfragen
                messages=[
                    {"role": "system", "content": "Du bist ein erfahrener Coach, der gezielt nachfragt."},
                    {"role": "user", "content": clarification_prompt}
                ],
                max_tokens=300,
                temperature=0.3
            )
            
            clarification = response.choices[0].message.content.strip()
            
            # Speichere in der Historie
            self.clarification_history.append({
                "original_question": question,
                "clarification": clarification,
                "timestamp": self._get_timestamp()
            })
            
            return clarification
            
        except Exception as e:
            logger.error(f"Clarification generation failed: {e}")
            return "KÃ¶nntest du deine Frage etwas spezifischer stellen? Was genau mÃ¶chtest du wissen?"
    
    def generate_answer_with_followup_questions(self, question: str, context_chunks: List[Dict[str, Any]], system_prompt: Optional[str] = None, conversation_history: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Generiert eine Antwort UND weitere Nachfragen fÃ¼r noch bessere Hilfe"""
        
        # Use more chunks for comprehensive answers (up to 30)
        max_chunks = min(30, len(context_chunks))
        selected_chunks = context_chunks[:max_chunks]
        
        # Baue Kontext fÃ¼r die Antwort
        context_text = self._build_context_for_clarification(selected_chunks)
        
        # Build conversation history context
        history_context = ""
        if conversation_history and len(conversation_history) > 0:
            history_parts = []
            for entry in conversation_history[-5:]:  # Last 5 conversations
                history_parts.append(f"Frage: {entry.get('question', '')}")
                history_parts.append(f"Antwort: {entry.get('answer', '')}")
            history_context = "\n\nVorherige Unterhaltung:\n" + "\n\n".join(history_parts)
        
        # Generiere Antwort mit dem bereitgestellten System-Prompt
        if system_prompt:
            answer_prompt = f"""Kontext aus dem Video:
{context_text}{history_context}

Frage: {question}

Antworte basierend auf dem bereitgestellten Kontext und der Unterhaltungshistorie. Gib eine umfassende, detaillierte Antwort. Verwende verschiedene Einleitungen und einen direkten, motivierenden Ton wie "BOOM, lasst es uns direkt angehen!" oder Ã¤hnliche Variationen. Wenn die Antwort nicht im Kontext gefunden werden kann, sage das ehrlich."""
        else:
            answer_prompt = f"""Du bist ein hilfreicher Assistent, der Fragen zu Video-Inhalten beantwortet.

Kontext aus dem Video:
{context_text}{history_context}

Frage: {question}

Antworte basierend auf dem bereitgestellten Kontext und der Unterhaltungshistorie. Gib eine umfassende, detaillierte Antwort. Verwende deutsche Sprache und sei prÃ¤zise. Verwende verschiedene motivierende Einleitungen wie "BOOM, lasst es uns direkt angehen!", "Perfekt, hier ist mein direkter Ansatz:", "Alles klar, lass uns das sofort anpacken:" oder Ã¤hnliche Variationen. BerÃ¼cksichtige die vorherige Unterhaltung fÃ¼r einen natÃ¼rlichen GesprÃ¤chsfluss."""

        try:
            # Generiere Antwort
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt or "Du bist ein hilfreicher Assistent fÃ¼r Video-Inhalte."},
                    {"role": "user", "content": answer_prompt}
                ],
                max_tokens=400,
                temperature=0.7  # HÃ¶here Temperature fÃ¼r mehr Variationen
            )
            
            answer = response.choices[0].message.content.strip()
            
            # Generiere weitere Nachfragen fÃ¼r noch bessere Hilfe
            followup_prompt = f"""Du bist ein erfahrener Coach. Du hast gerade eine Antwort gegeben, aber mÃ¶chtest noch gezielter helfen.

UrsprÃ¼ngliche Frage: "{question}"
Deine Antwort: "{answer}"

VerfÃ¼gbarer Kontext aus den Videos:
{context_text}{history_context}

Deine Aufgabe: Stelle 2-3 zusÃ¤tzliche Nachfragen, die dem Fragesteller helfen, sein Problem noch besser zu lÃ¶sen. Diese sollten tiefer gehen als die ursprÃ¼ngliche Frage und die Unterhaltung natÃ¼rlich fortsetzen.

BerÃ¼cksichtige die vorherige Unterhaltung, um:
- Nicht bereits besprochene Themen zu vermeiden
- Auf vorherige Antworten aufzubauen
- Die Unterhaltung logisch fortzusetzen

Beispiele:
- Bei Gewichtsabnahme: "Wie ist dein Schlafrhythmus? Trinkst du genug Wasser? Hast du Stress?"
- Bei Lead-Generierung: "Wie ist deine aktuelle Website? Nutzt du Social Media? Hast du ein Budget?"

Stelle die Nachfragen in einem freundlichen, aber direkten Ton. Verwende "du" und sei konkret. Verwende verschiedene Formulierungen und Emojis fÃ¼r Abwechslung.
Antworte NUR mit den Nachfragen, keine zusÃ¤tzlichen ErklÃ¤rungen."""

            followup_response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "Du bist ein erfahrener Coach, der gezielt nachfragt."},
                    {"role": "user", "content": followup_prompt}
                ],
                max_tokens=200,
                temperature=0.8  # HÃ¶here Temperature fÃ¼r mehr Variationen in Nachfragen
            )
            
            followup_questions = followup_response.choices[0].message.content.strip()
            
            return {
                "answer": answer,
                "followup_questions": followup_questions,
                "context_chunks_used": len(selected_chunks),
                "total_chunks_found": len(context_chunks)
            }
            
        except Exception as e:
            logger.error(f"Answer with followup generation failed: {e}")
            return {
                "answer": "Entschuldigung, ich konnte keine Antwort generieren.",
                "followup_questions": "KÃ¶nntest du deine Frage etwas spezifischer stellen?",
                "context_chunks_used": 0,
                "total_chunks_found": len(context_chunks)
            }
    
    def _build_context_for_clarification(self, chunks: List[Dict[str, Any]]) -> str:
        """Baut Kontext fÃ¼r die Nachfrage-Generierung"""
        
        if not chunks:
            return "Kein spezifischer Kontext verfÃ¼gbar."
        
        # Use more chunks for better context (up to 15 for clarification)
        max_chunks = min(15, len(chunks))
        context_chunks = chunks[:max_chunks]
        
        context_parts = []
        for i, chunk in enumerate(context_chunks, 1):
            text = chunk.get('chunk_text', '')
            speaker = chunk.get('speaker', 'Unknown')
            
            context_parts.append(f"[{speaker}]: {text}")
        
        return "\n\n".join(context_parts)
    
    def _get_timestamp(self) -> str:
        """Get current timestamp"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def _get_random_answer_intro(self) -> str:
        """Gibt eine zufÃ¤llige Einleitung fÃ¼r Antworten zurÃ¼ck"""
        intros = [
            "BOOM, lasst es uns direkt angehen!",
            "Perfekt, hier ist mein direkter Ansatz:",
            "Alles klar, lass uns das sofort anpacken:",
            "Genau das brauchst du - hier ist mein Plan:",
            "Super Frage! Lass mich dir das direkt erklÃ¤ren:",
            "Okay, hier ist mein ehrlicher Ansatz:",
            "Das ist ein wichtiges Thema - lass uns das richtig angehen:",
            "Verstehe! Hier ist mein direkter Ratschlag:",
            "Gute Frage! Lass mich dir das sofort zeigen:",
            "Hier ist mein ehrlicher Take dazu:",
            "Das ist genau das, was du brauchst:",
            "Lass uns das direkt und ehrlich angehen:",
            "Hier ist mein direkter Ansatz fÃ¼r dich:",
            "Perfekt! Lass mich dir das sofort erklÃ¤ren:",
            "Das ist ein wichtiger Punkt - hier ist mein Plan:"
        ]
        return random.choice(intros)
    
    def _get_random_followup_intro(self) -> str:
        """Gibt eine zufÃ¤llige Einleitung fÃ¼r Nachfragen zurÃ¼ck"""
        intros = [
            "ğŸ¤” Um dir noch besser helfen zu kÃ¶nnen, habe ich noch ein paar weitere Fragen:",
            "ğŸ’¡ Lass mich noch ein paar wichtige Details wissen:",
            "ğŸ¯ Um dir gezielter helfen zu kÃ¶nnen, brauche ich noch:",
            "âš¡ FÃ¼r eine noch bessere Antwort, erzÃ¤hl mir noch:",
            "ğŸ”¥ Um das richtig zu lÃ¶sen, brauche ich noch:",
            "ğŸ’ª FÃ¼r den perfekten Plan, sag mir noch:",
            "ğŸš€ Um dir optimal zu helfen, erzÃ¤hl mir:",
            "âœ¨ FÃ¼r eine maÃŸgeschneiderte LÃ¶sung, brauche ich:",
            "ğŸª Um das richtig anzugehen, sag mir noch:",
            "ğŸ’ FÃ¼r die beste Strategie, erzÃ¤hl mir:",
            "ğŸ”¥ Um das richtig zu rocken, brauche ich noch:",
            "âš¡ FÃ¼r den perfekten Durchbruch, sag mir:",
            "ğŸ¯ Um dir gezielt zu helfen, erzÃ¤hl mir noch:",
            "ğŸ’ª FÃ¼r den besten Ansatz, brauche ich:",
            "ğŸš€ Um das richtig zu lÃ¶sen, sag mir noch:"
        ]
        return random.choice(intros)
    
    def get_clarification_history(self) -> List[Dict[str, Any]]:
        """Gibt die Nachfrage-Historie zurÃ¼ck"""
        return self.clarification_history
    
    def check_if_ready_for_final_answer(self, conversation_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Analysiert die Konversationshistorie und entscheidet, ob genug SpezifitÃ¤t vorhanden ist
        fÃ¼r eine vollstÃ¤ndige Antwort, oder ob noch eine Nachfrage nÃ¶tig ist.
        
        Returns:
            Dict mit 'ready' (bool) und optional 'reason' (str)
        """
        if not conversation_history:
            return {"ready": False, "reason": "no_history"}
        
        # Build conversation context
        conversation_text = []
        for entry in conversation_history:
            conversation_text.append(f"Frage: {entry.get('question', '')}")
            conversation_text.append(f"Antwort: {entry.get('answer', '')}")
        
        conversation_context = "\n".join(conversation_text)
        
        # Ask GPT-4o to analyze if we have enough information
        analysis_prompt = f"""Du bist ein Experte darin zu entscheiden, ob eine Unterhaltung genug SpezifitÃ¤t erreicht hat, um eine vollstÃ¤ndige, hilfreiche Antwort zu geben.

Analysiere die folgende Unterhaltung:

{conversation_context}

Deine Aufgabe: Entscheide, ob wir jetzt genug spezifische Informationen haben, um eine wirklich gute, umfassende Antwort zu geben, oder ob wir noch mehr Details brauchen.

Eine Antwort ist bereit, wenn:
- Konkrete, messbare Details vorhanden sind (Zahlen, ZeitrÃ¤ume, etc.)
- Der Kontext klar ist (Situation, Ziel, Rahmenbedingungen)
- Mindestens 2-3 spezifische Aspekte genannt wurden
- Die Frage so konkret ist, dass man eine maÃŸgeschneiderte LÃ¶sung geben kann

Eine Antwort ist NICHT bereit, wenn:
- Nur vage Aussagen gemacht wurden
- Wichtige Details fehlen (Budget, Zeitrahmen, aktuelle Situation, etc.)
- Weniger als 2-3 Nachfragen beantwortet wurden
- Die Antworten sehr kurz oder unspezifisch sind

Antworte NUR mit einem JSON-Objekt in folgendem Format:
{{
  "ready": true/false,
  "confidence": 0.0-1.0,
  "missing_info": "Was fehlt noch?" (nur wenn ready=false),
  "reason": "Kurze BegrÃ¼ndung"
}}"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "Du bist ein Experte fÃ¼r GesprÃ¤chsanalyse."},
                    {"role": "user", "content": analysis_prompt}
                ],
                max_tokens=200,
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            import json
            result = json.loads(response.choices[0].message.content.strip())
            logger.info(f"Readiness check: {result}")
            return result
            
        except Exception as e:
            logger.error(f"Readiness check failed: {e}")
            # Default: not ready after less than 3 exchanges
            return {
                "ready": len(conversation_history) >= 3,
                "confidence": 0.5,
                "reason": "Default check based on conversation length"
            }
    
    def generate_single_clarification_question(self, question: str, conversation_history: List[Dict[str, Any]], context_chunks: List[Dict[str, Any]]) -> str:
        """
        Generiert EINE gezielte Nachfrage basierend auf der bisherigen Unterhaltung.
        """
        # Build conversation context
        conversation_text = []
        for entry in conversation_history:
            conversation_text.append(f"Frage: {entry.get('question', '')}")
            conversation_text.append(f"Antwort: {entry.get('answer', '')}")
        
        conversation_context = "\n".join(conversation_text) if conversation_text else "Keine vorherige Unterhaltung"
        
        # Build context from chunks
        context_text = self._build_context_for_clarification(context_chunks)
        
        clarification_prompt = f"""Du bist ein erfahrener Coach, der durch gezielte Einzelfragen mehr Details erfÃ¤hrt.

Bisherige Unterhaltung:
{conversation_context}

Aktuelle Frage: "{question}"

VerfÃ¼gbarer Kontext aus Videos:
{context_text[:1000]}

Deine Aufgabe: Stelle EINE gezielte, konkrete Nachfrage, um mehr spezifische Details zu erfahren. 

Die Nachfrage sollte:
- Auf das bisher Gesagte aufbauen
- Nach konkreten, messbaren Details fragen (Zahlen, ZeitrÃ¤ume, Budget, etc.)
- Kurz und prÃ¤zise sein
- Dem Nutzer helfen, sein Problem besser zu formulieren

Beispiele fÃ¼r gute Nachfragen:
- "Wie viel Budget hast du dafÃ¼r zur VerfÃ¼gung?"
- "Wie viele Stunden pro Woche kannst du dafÃ¼r investieren?"
- "Was hast du bisher schon versucht?"
- "Bis wann mÃ¶chtest du dieses Ziel erreichen?"

Verwende einen freundlichen, direkten Ton mit "du". 
Antworte NUR mit der Nachfrage, keine ErklÃ¤rungen."""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "Du bist ein erfahrener Coach."},
                    {"role": "user", "content": clarification_prompt}
                ],
                max_tokens=150,
                temperature=0.4
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"Single clarification generation failed: {e}")
            return "Kannst du mir mehr Details dazu geben?"


class MiniChatAgent:
    """Mini chat agent for video content Q&A"""
    
    def __init__(self):
        """Initialize mini chat agent"""
        self.openai_client = OpenAI(api_key=settings.openai_api_key)
        self.video_processor = VideoProcessor()
        self.conversation_history = []
        
        # Initialize clarification mode
        self.clarification_mode = ClarificationMode(self.openai_client, self.video_processor)
        self.clarification_mode_enabled = True  # Automatisch aktiviert
        self.iterative_clarification_mode = False  # One-question-at-a-time mode
        
        logger.info("Initialized MiniChatAgent with ClarificationMode")
    
    def ask_question(self, question: str, video_id: Optional[str] = None, 
                    context_limit: int = 20, system_prompt: Optional[str] = None) -> Dict[str, Any]:
        """
        Ask a question about video content with clarification mode
        
        Args:
            question: User's question
            video_id: Optional video ID to limit search
            context_limit: Number of relevant chunks to include
            system_prompt: Optional custom system prompt
            
        Returns:
            Response with answer and sources
        """
        logger.info(f"Processing question: '{question}'")
        
        try:
            # Search for relevant chunks first
            relevant_chunks = self.video_processor.search_video_content(
                question, video_id
            )
            
            # ===== ITERATIVE CLARIFICATION MODE =====
            if self.iterative_clarification_mode and relevant_chunks:
                logger.info("Iterative clarification mode active")
                
                # Check if we have enough specificity for a final answer
                readiness_check = self.clarification_mode.check_if_ready_for_final_answer(
                    self.conversation_history
                )
                
                if readiness_check.get('ready', False):
                    # We have enough information - generate comprehensive answer
                    logger.info("Ready for final answer - generating comprehensive response")
                    
                    # Use up to 30 chunks for comprehensive answer
                    context_chunks = relevant_chunks[:30]
                    context_text = self._build_context(context_chunks)
                    
                    # Build full conversation context
                    conversation_text = []
                    for entry in self.conversation_history:
                        conversation_text.append(f"Frage: {entry.get('question', '')}")
                        conversation_text.append(f"Antwort: {entry.get('answer', '')}")
                    conversation_context = "\n\n".join(conversation_text)
                    
                    # Generate comprehensive answer
                    final_prompt = f"""Basierend auf der gesamten Unterhaltung, gib jetzt eine umfassende, detaillierte und hilfreiche Antwort.

Bisherige Unterhaltung:
{conversation_context}

Aktuelle Frage: {question}

VerfÃ¼gbare Informationen aus Videos:
{context_text}

Gib eine vollstÃ¤ndige, maÃŸgeschneiderte Antwort basierend auf allen gesammelten Informationen. Sei spezifisch, konkret und hilfreich."""

                    if system_prompt:
                        system_content = system_prompt
                    else:
                        system_content = "Du bist ein hilfreicher Experte, der umfassende, maÃŸgeschneiderte LÃ¶sungen gibt."
                    
                    response_obj = self.openai_client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "system", "content": system_content},
                            {"role": "user", "content": final_prompt}
                        ],
                        max_tokens=600,
                        temperature=0.7
                    )
                    
                    final_answer = response_obj.choices[0].message.content.strip()
                    confidence = self._calculate_confidence(context_chunks, question)
                    
                    # Add to history
                    self.conversation_history.append({
                        "question": question,
                        "answer": final_answer,
                        "timestamp": self._get_timestamp()
                    })
                    
                    return {
                        "answer": final_answer,
                        "sources": self._format_sources(context_chunks),
                        "all_selected_chunks": self._format_sources(relevant_chunks),
                        "used_chunk_indices": list(range(len(context_chunks))),
                        "confidence": confidence,
                        "context_chunks_used": len(context_chunks),
                        "total_chunks_found": len(relevant_chunks),
                        "clarification_mode": True,
                        "iterative_mode": True,
                        "final_answer": True
                    }
                    
                else:
                    # Not ready yet - ask ONE clarification question
                    logger.info(f"Not ready for final answer: {readiness_check.get('reason', 'unknown')}")
                    
                    single_question = self.clarification_mode.generate_single_clarification_question(
                        question, self.conversation_history, relevant_chunks
                    )
                    
                    confidence = self._calculate_confidence(relevant_chunks[:10], question)
                    
                    # Random intro for single question
                    intros = [
                        "ğŸ¤” Interessant! Lass mich noch eine Sache wissen:",
                        "ğŸ’¡ Um dir besser zu helfen, beantworte mir noch:",
                        "ğŸ¯ Perfekt, eine Frage noch:",
                        "âš¡ Fast da! Noch eine wichtige Info:",
                        "ğŸ”¥ Super, sag mir noch:",
                        "ğŸ’ª Okay! Eine Sache noch:",
                        "ğŸš€ Verstehe! Lass mich noch wissen:"
                    ]
                    intro = random.choice(intros)
                    
                    answer = f"{intro}\n\n{single_question}"
                    
                    # Add to history
                    self.conversation_history.append({
                        "question": question,
                        "answer": answer,
                        "timestamp": self._get_timestamp()
                    })
                    
                    return {
                        "answer": answer,
                        "sources": self._format_sources(relevant_chunks[:10]),
                        "all_selected_chunks": self._format_sources(relevant_chunks),
                        "used_chunk_indices": list(range(min(10, len(relevant_chunks)))),
                        "confidence": confidence,
                        "context_chunks_used": 10,
                        "total_chunks_found": len(relevant_chunks),
                        "clarification_mode": True,
                        "iterative_mode": True,
                        "final_answer": False
                    }
            
            # ===== ORIGINAL CLARIFICATION MODE =====
            # Check if clarification mode is enabled
            if self.clarification_mode_enabled and relevant_chunks and not self.iterative_clarification_mode:
                
                # Check if this is a very vague question (first time asking) AND no conversation history
                if (self.clarification_mode.is_question_too_vague(question) and 
                    len(self.conversation_history) == 0):
                    logger.info("Question detected as very vague with no history, generating initial clarification questions")
                    
                    # Generate initial clarification questions
                    clarification = self.clarification_mode.generate_clarification_questions(
                        question, relevant_chunks
                    )
                    
                    # Calculate confidence based on found chunks
                    confidence = self._calculate_confidence(relevant_chunks[:10], question)
                    
                    # ZufÃ¤llige Einleitung fÃ¼r Nachfragen
                    intro_variations = [
                        "ğŸ¤” Deine Frage ist noch etwas unspezifisch. Um dir die beste Antwort zu geben, brauche ich mehr Details:",
                        "ğŸ’¡ Lass mich das besser verstehen. FÃ¼r eine gezielte Antwort brauche ich noch:",
                        "ğŸ¯ Um dir optimal helfen zu kÃ¶nnen, erzÃ¤hl mir noch:",
                        "âš¡ FÃ¼r die beste LÃ¶sung brauche ich noch ein paar Details:",
                        "ğŸ”¥ Um das richtig anzugehen, sag mir noch:",
                        "ğŸ’ª FÃ¼r den perfekten Plan brauche ich:",
                        "ğŸš€ Um dir gezielt zu helfen, erzÃ¤hl mir:",
                        "âœ¨ FÃ¼r eine maÃŸgeschneiderte Antwort, sag mir:"
                    ]
                    intro = random.choice(intro_variations)
                    
                    return {
                        "answer": f"{intro}\n\n{clarification}\n\nBitte beantworte diese Fragen, dann kann ich dir eine gezielte Antwort geben!",
                        "sources": self._format_sources(relevant_chunks[:10]),
                        "all_selected_chunks": self._format_sources(relevant_chunks),
                        "used_chunk_indices": list(range(min(10, len(relevant_chunks)))),
                        "confidence": confidence,
                        "context_chunks_used": 10,
                        "total_chunks_found": len(relevant_chunks),
                        "clarification_mode": True,
                        "clarification_questions": clarification
                    }
                
                # For all other questions in clarification mode (including follow-up answers), provide answer + followup questions
                else:
                    logger.info("Generating answer with followup questions for clarification mode")
                    
                    # Generate answer with followup questions
                    result = self.clarification_mode.generate_answer_with_followup_questions(
                        question, relevant_chunks, system_prompt, self.conversation_history
                    )
                    
                    # Calculate confidence using more chunks
                    confidence = self._calculate_confidence(relevant_chunks[:30], question)
                    
                    # Add to conversation history
                    self.conversation_history.append({
                        "question": question,
                        "answer": result['answer'],
                        "timestamp": self._get_timestamp()
                    })
                    
                    # ZufÃ¤llige Einleitung fÃ¼r Nachfragen
                    followup_intro = self.clarification_mode._get_random_followup_intro()
                    
                    return {
                        "answer": f"{result['answer']}\n\n{followup_intro}\n\n{result['followup_questions']}\n\nBitte beantworte diese, dann kann ich dir noch gezielter helfen!",
                        "sources": self._format_sources(relevant_chunks[:30]),
                        "all_selected_chunks": self._format_sources(relevant_chunks),
                        "used_chunk_indices": list(range(min(30, len(relevant_chunks)))),
                        "confidence": confidence,
                        "context_chunks_used": result['context_chunks_used'],
                        "total_chunks_found": result['total_chunks_found'],
                        "clarification_mode": True,
                        "followup_questions": result['followup_questions']
                    }
            
            # If question is specific enough or clarification mode is disabled, proceed normally
            if not relevant_chunks:
                return {
                    "answer": "Entschuldigung, ich konnte keine relevanten Informationen zu Ihrer Frage finden.",
                    "sources": [],
                    "all_selected_chunks": [],
                    "used_chunk_indices": [],
                    "confidence": 0.0,
                    "context_chunks_used": 0,
                    "total_chunks_found": 0
                }
            
            # Limit context to avoid token limits
            context_chunks = relevant_chunks[:context_limit]
            
            # Build context for LLM
            context_text = self._build_context(context_chunks)
            
            # Generate answer using LLM
            answer = self._generate_answer(question, context_text, system_prompt)
            
            # Calculate confidence based on chunk relevance
            confidence = self._calculate_confidence(context_chunks, question)
            
            # Prepare response
            response = {
                "answer": answer,
                "sources": self._format_sources(context_chunks),
                "all_selected_chunks": self._format_sources(relevant_chunks),  # All chunks found
                "used_chunk_indices": list(range(len(context_chunks))),  # Indices of chunks actually used
                "confidence": confidence,
                "context_chunks_used": len(context_chunks),
                "total_chunks_found": len(relevant_chunks),
                "clarification_mode": False
            }
            
            # Add to conversation history
            self.conversation_history.append({
                "question": question,
                "answer": answer,
                "timestamp": self._get_timestamp()
            })
            
            logger.info(f"Generated answer with confidence: {confidence:.2f}")
            return response
            
        except Exception as e:
            logger.error(f"Failed to process question: {e}")
            return {
                "answer": f"Entschuldigung, es ist ein Fehler aufgetreten: {str(e)}",
                "sources": [],
                "all_selected_chunks": [],
                "used_chunk_indices": [],
                "confidence": 0.0,
                "context_chunks_used": 0,
                "total_chunks_found": 0
            }
    
    def _build_context(self, chunks: List[Dict[str, Any]]) -> str:
        """Build context text from relevant chunks"""
        
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            text = chunk.get('chunk_text', '')
            timestamp = chunk.get('start_timestamp', 0)
            speaker = chunk.get('speaker', 'Unknown')
            
            context_parts.append(
                f"[Chunk {i} - {timestamp:.1f}s - {speaker}]: {text}"
            )
        
        return "\n\n".join(context_parts)
    
    def _generate_answer(self, question: str, context: str, system_prompt: Optional[str] = None) -> str:
        """Generate answer using OpenAI LLM"""
        
        # Use custom system prompt if provided, otherwise use default
        if system_prompt:
            system_content = system_prompt
            user_prompt = f"""Kontext aus dem Video:
{context}

Frage: {question}

Antworte basierend auf dem bereitgestellten Kontext. Wenn die Antwort nicht im Kontext gefunden werden kann, sage das ehrlich."""
        else:
            system_content = "Du bist ein hilfreicher Assistent fÃ¼r Video-Inhalte."
            user_prompt = f"""Du bist ein hilfreicher Assistent, der Fragen zu Video-Inhalten beantwortet.

Kontext aus dem Video:
{context}

Frage: {question}

Antworte basierend auf dem bereitgestellten Kontext. Wenn die Antwort nicht im Kontext gefunden werden kann, sage das ehrlich. Verwende deutsche Sprache und sei prÃ¤zise."""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",  # Cost-effective model
                messages=[
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=500,
                temperature=0.1
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"LLM generation failed: {e}")
            return "Entschuldigung, ich konnte keine Antwort generieren."
    
    def _format_sources(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Format sources for response"""
        
        sources = []
        for chunk in chunks:
            source = {
                "text": chunk.get('chunk_text', ''),  # Return full text, not truncated
                "timestamp": chunk.get('start_timestamp', 0),
                "speaker": chunk.get('speaker', 'Unknown'),
                "video_id": chunk.get('video_id', 'Unknown')
            }
            sources.append(source)
        
        return sources
    
    def _calculate_confidence(self, chunks: List[Dict[str, Any]], question: str) -> float:
        """Calculate confidence based on chunk relevance"""
        
        if not chunks:
            return 0.0
        
        # Simple confidence calculation based on number of chunks found
        # In a real implementation, you'd use semantic similarity scores
        base_confidence = min(len(chunks) / 3.0, 1.0)  # Max confidence with 3+ chunks
        
        # Adjust based on chunk quality (length, speaker info, etc.)
        quality_bonus = 0.0
        for chunk in chunks:
            if chunk.get('speaker'):
                quality_bonus += 0.1
            if len(chunk.get('chunk_text', '')) > 100:
                quality_bonus += 0.1
        
        final_confidence = min(base_confidence + quality_bonus, 1.0)
        return round(final_confidence, 2)
    
    def _get_timestamp(self) -> str:
        """Get current timestamp"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """Get conversation history"""
        return self.conversation_history
    
    def clear_history(self):
        """Clear conversation history"""
        self.conversation_history = []
        logger.info("Conversation history cleared")
    
    def toggle_clarification_mode(self, enabled: bool = None) -> bool:
        """Toggle clarification mode on/off"""
        if enabled is None:
            self.clarification_mode_enabled = not self.clarification_mode_enabled
        else:
            self.clarification_mode_enabled = enabled
        
        logger.info(f"Clarification mode {'enabled' if self.clarification_mode_enabled else 'disabled'}")
        return self.clarification_mode_enabled
    
    def get_clarification_history(self) -> List[Dict[str, Any]]:
        """Get clarification history"""
        return self.clarification_mode.get_clarification_history()
    
    def is_clarification_mode_enabled(self) -> bool:
        """Check if clarification mode is enabled"""
        return self.clarification_mode_enabled
    
    def toggle_iterative_clarification_mode(self, enabled: bool = None) -> bool:
        """Toggle iterative clarification mode on/off"""
        if enabled is None:
            self.iterative_clarification_mode = not self.iterative_clarification_mode
        else:
            self.iterative_clarification_mode = enabled
        
        logger.info(f"Iterative clarification mode {'enabled' if self.iterative_clarification_mode else 'disabled'}")
        return self.iterative_clarification_mode
    
    def is_iterative_clarification_mode_enabled(self) -> bool:
        """Check if iterative clarification mode is enabled"""
        return self.iterative_clarification_mode


class InteractiveChatSession:
    """Interactive chat session for testing"""
    
    def __init__(self, video_id: Optional[str] = None):
        """Initialize chat session"""
        self.agent = MiniChatAgent()
        self.video_id = video_id
        
        print("ğŸ¤– Mini Chat Agent initialized!")
        if video_id:
            print(f"ğŸ“¹ Focused on video: {video_id}")
        print("ğŸ’¡ Type 'quit' to exit, 'history' to see conversation, 'clear' to clear history")
        print("-" * 60)
    
    def start_session(self):
        """Start interactive chat session"""
        
        while True:
            try:
                question = input("\nâ“ Your question: ").strip()
                
                if question.lower() == 'quit':
                    print("ğŸ‘‹ Goodbye!")
                    break
                elif question.lower() == 'history':
                    self._show_history()
                    continue
                elif question.lower() == 'clear':
                    self.agent.clear_history()
                    print("ğŸ§¹ History cleared!")
                    continue
                elif not question:
                    continue
                
                # Process question
                print("ğŸ” Searching for relevant content...")
                response = self.agent.ask_question(question, self.video_id)
                
                # Display response
                print(f"\nğŸ¤– Answer:")
                print(f"   {response['answer']}")
                
                print(f"\nğŸ“Š Confidence: {response['confidence']}")
                
                # Handle the context_chunks_used field properly
                if 'context_chunks_used' in response:
                    print(f"ğŸ“š Sources used: {response['context_chunks_used']}/{response['total_chunks_found']}")
                else:
                    print(f"ğŸ“š Sources used: {len(response.get('sources', []))}/{response.get('total_chunks_found', 0)}")
                
                if response.get('sources'):
                    print(f"\nğŸ“ Sources:")
                    for i, source in enumerate(response['sources'], 1):
                        print(f"   {i}. [{source['timestamp']:.1f}s] {source['speaker']}: {source['text']}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Goodbye!")
                break
            except Exception as e:
                print(f"âŒ Error: {e}")
    
    def _show_history(self):
        """Show conversation history"""
        history = self.agent.get_conversation_history()
        
        if not history:
            print("ğŸ“ No conversation history yet.")
            return
        
        print(f"\nğŸ“ Conversation History ({len(history)} exchanges):")
        print("-" * 40)
        
        for i, exchange in enumerate(history, 1):
            print(f"{i}. Q: {exchange['question']}")
            print(f"   A: {exchange['answer'][:100]}...")
            print(f"   Time: {exchange['timestamp']}")
            print()
